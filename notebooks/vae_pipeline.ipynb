{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Project Imports",
   "id": "7ded3c9d458a5f2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import minimal_version.parser_handler\n",
    "\n",
    "from minimal_version.preprocess_msa import Preprocessor, weight_sequences\n",
    "from minimal_version.msa import MSA\n",
    "from minimal_version.utils import Capturing, store_to_pkl, store_to_fasta, load_from_pkl\n",
    "from minimal_version.hmm_msa_preprocessing import HMMAligner\n",
    "\n",
    "from minimal_version.train import setup_train, Train"
   ],
   "id": "ac3db5c55f2bd64a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Project Configuration\n",
    "Configure your project using the configuration file. Use the modelConfig template as a guide. \n",
    "\n",
    "A value of -1 indicates that the default value will be used. \n",
    "\n",
    "Please note that every time the RunSetup class is executed, the current version of the configuration file will be copied to your specified directory, allowing you to review all set parameters at any time.  \n",
    "\n",
    "Run just one of the two cell below (working with/without Pfam)"
   ],
   "id": "52a3ea326c408c04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CONFIGURATION_FILE = \"msaExample.json\"\n",
    "run = minimal_version.parser_handler.RunSetup(CONFIGURATION_FILE)\n",
    "print(f\" Working with {CONFIGURATION_FILE} configuration file!\")\n",
    "PFAM_INPUT = False"
   ],
   "id": "a7efbcc7f5166be5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CONFIGURATION_FILE = \"pfamExample.json\"\n",
    "run = minimal_version.parser_handler.RunSetup(CONFIGURATION_FILE)\n",
    "print(f\" Working with {CONFIGURATION_FILE} configuration file!\")\n",
    "PFAM_INPUT = True"
   ],
   "id": "730296bc5073d5ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### HMM alignment preprocessing\n",
    "This preprocessing step generates a Hidden Markov Model (HMM) profile from a Multiple Sequence Alignment (MSA) and realigns the sequences from the MSA to this profile, resulting in a HMM-MSA. \n",
    "\n",
    "The HMM-MSA can then be utilized as a classical input MSA in the Variational Autoencoder (VAE) pipeline, offering the advantage of facilitating the alignment of different query sequences to the HMM profile. If this option is enabled, ensure that the dataset path in the configuration file is updated to the new fixed path to maintain consistency in future experiments.\n",
    "\n",
    "Install hmmer to your system first: for example **apt install hmmer** "
   ],
   "id": "727b4a3f71b6ea8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If you want to use hmm aligner run this cell\n",
    "hmmAligner = HMMAligner(run)\n",
    "hmmAligner.buildHMM()  # you can specify your custom msa path to build HMM model on "
   ],
   "id": "3a2497b9ff0ef873",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "hmmAligner.hmmer_align(\"yourCustomFasta\")  # you can align your custom sequences to the hmm model here ",
   "id": "8c93782cf538a236",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MSA preprocessing\n",
    "Prepare your MSA for training by filtering it according to the queries specified in the configuration file. This includes both the query sequences and any fixed sequences defined."
   ],
   "id": "3635675ba628f4ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f278cf01cf260f49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Preprocess MSA and prepare it for VAE model training\n",
    "\"\"\"\n",
    "\n",
    "# logging\n",
    "msa_log = open(os.path.join(run.logs, 'msa_log.txt'), \"w\")\n",
    "\n",
    "# MSA loading\n",
    "if PFAM_INPUT:\n",
    "    msa = MSA.load_pfam(run.dataset)\n",
    "else:\n",
    "    msa = MSA.load_msa(run.dataset)\n",
    "MSA.amino_acid_dict(run.pickles)\n",
    "msg = f\"MSA in {run.dataset} loaded\\n\" \\\n",
    "      f\"number of sequences: {len(list(msa.keys()))}\"\n",
    "print(msg)\n",
    "msa_log.write(msg + f'\\n{\"=\"*80}\\n')\n",
    "\n",
    "# MSA preprocessing\n",
    "preprocessor = Preprocessor(run)\n",
    "with Capturing() as output:\n",
    "    msa, msa_keys = preprocessor.trim_msa(msa)\n",
    "msa_log.write(\" MSA Preprocessing \\n\" + \"\\n\".join(output) + f'\\n{\"=\"*80}\\n')\n",
    "assert (msa.shape[0] == len(msa_keys))\n",
    "trim_file_path = os.path.join(run.msa, \"trimmed_msa.fasta\")\n",
    "msg = f\"Trimmed MSA has {msa.shape[0]} sequences and the width is {msa.shape[1]}\\n\" \\\n",
    "      f\"Trimmed MSA is stored at {trim_file_path}\"\n",
    "print(msg)\n",
    "msa_log.write(msg + f'\\n{\"=\"*80}\\n')\n",
    "msa_num_dict_shuffled = {k: seq for k, seq in zip(msa_keys, msa)}  # transform to dictionary, have keys together\n",
    "# but secure that query and fixed sequences are at the beginning \n",
    "msa_num_dict = {k: msa_num_dict_shuffled[k] for k in preprocessor.keep_keys}\n",
    "msa_num_dict.update({k: msa_num_dict_shuffled[k] for k in msa_num_dict_shuffled if k not in preprocessor.keep_keys})\n",
    "\n",
    "trimmed_msa = MSA.number_to_amino(msa_num_dict)\n",
    "store_to_fasta(trimmed_msa, trim_file_path)\n",
    "store_to_pkl({run.query: trimmed_msa[run.query]}, os.path.join(run.pickles, \"reference_seq.pkl\"))\n",
    "\n",
    "# Filtering or weighting\n",
    "with Capturing() as output:\n",
    "    if run.clustering:  # MSA filtering\n",
    "        print(f\"MSA {run.identity}% identity filtering step\")\n",
    "        msa_num_dict = preprocessor.identity_filtering(msa_num_dict)\n",
    "        msa, training_alg = preprocessor.get_keys_file_and_np_sequences(msa_num_dict)  # Overlap one treemmer\n",
    "        seq_weight = np.ones(msa.shape[0])  # we just provide uniform weights for all sequences\n",
    "    else:  # otherwise the weighting mechanism will be applied\n",
    "        msa, training_alg = preprocessor.get_keys_file_and_np_sequences(msa_num_dict)  # Overlap one treemmer\n",
    "        seq_weight = weight_sequences(msa)\n",
    "msa_log.write(\"\\n\".join(output))\n",
    "train_msa_file_path = os.path.join(run.msa, \"training_msa.fasta\")\n",
    "training_alg = MSA.number_to_amino(training_alg)\n",
    "store_to_fasta(training_alg, train_msa_file_path)\n",
    "msg = f\"Training MSA has {msa.shape[0]} sequences and the width is {msa.shape[1]}\\n\" \\\n",
    "      f\"Training MSA is stored at {train_msa_file_path}\"\n",
    "print(msg)\n",
    "msa_log.write(msg + f'\\n{\"=\" * 80}\\n')\n",
    "\n",
    "store_to_pkl(seq_weight, os.path.join(run.pickles, \"seq_weight.pkl\"))\n",
    "store_to_pkl(training_alg, os.path.join(run.pickles, \"training_alignment.pkl\"))\n",
    "\n",
    "# MSA one-hot encoding\n",
    "binary = MSA.number_to_binary(msa)\n",
    "store_to_pkl(binary, os.path.join(run.pickles, \"seq_msa_binary.pkl\"))\n",
    "msa_log.close()\n"
   ],
   "id": "ef7b64db1b86d7a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Conditional labels\n",
    "Before starting the training process, ensure that your labels are properly preprocessed if you intend to use them for **conditional training**. \n",
    "\n",
    "This involves converting your labels into a suitable format, such as one-hot or integer encoding (see and modify **custom_classifier** function). \n",
    "\n",
    "Additionally, specify the number of classes represented by your labels."
   ],
   "id": "b82b8052e59434e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify number of classes\n",
    "N_CLASSES = 3\n",
    "\n",
    "def parse_conditional_labels_to_categories(file_name):\n",
    "    \"\"\" Parse conditional label file \"\"\"\n",
    "    # Please specify your classification function to the bins, the interface of function must stay the same, example\n",
    "    def custom_classifier(sol_val: float) -> torch.Tensor:\n",
    "        if sol_val < run.solubility_threshold1:\n",
    "            return torch.tensor([1,0,0])\n",
    "        if sol_val <= run.solubility_threshold2:\n",
    "            return torch.tensor([0,1,0])\n",
    "        return torch.tensor([0,0,1])\n",
    "    \n",
    "    label_dict = {}\n",
    "    with open(file_name, \"r\") as sol_data:\n",
    "        for i, record in enumerate(sol_data):\n",
    "            if i == 0:  # header\n",
    "                continue\n",
    "            accession, label_val = record.split(\"\\t\")\n",
    "            if label_val is None or len(label_val) == '':\n",
    "                print(f\"Error while parsing labels, accession {accession} does not have labels\")\n",
    "                exit(1)\n",
    "            label_dict[accession] = custom_classifier(float(label_val))\n",
    "    return label_dict\n",
    "\n",
    "\"\"\" In the case of conditional model please preprocess labels \"\"\"\n",
    "label_file_path = run.conditional_labels  # you can specify it in the configuration path or provide custom one \n",
    "\n",
    "# Now get labels from your file, simple txt or similar file with sequence accession\\tlabel_value pair per line (modify function as needed)  \n",
    "label_dict = parse_conditional_labels_to_categories(label_file_path)\n",
    "\n",
    "labels_file_pkl = os.path.join(run.conditional_data, \"conditional_labels_to_categories.pkl\")\n",
    "msa_keys = load_from_pkl(os.path.join(run.pickles, \"keys_list.pkl\"))\n",
    "\n",
    "# get labels to the similar order as in msa (just in case, to enable using indexes in training method)\n",
    "msa_key_label_order = {}\n",
    "array_of_labels = torch.zeros(len(msa_keys), N_CLASSES)\n",
    "for label_i, msa_key in enumerate(msa_keys):\n",
    "    try:\n",
    "        msa_key_label_order[msa_key] = label_dict[msa_key]\n",
    "        array_of_labels[label_i] = label_dict[msa_key]\n",
    "    except KeyError:\n",
    "        print(f\"{msa_key} is not in the label_dict, please provide labels for all sequences\")\n",
    "        exit(1)\n",
    "        \n",
    "# now store in both raw and text version\n",
    "store_to_pkl(array_of_labels, labels_file_pkl)\n",
    "with open(os.path.join(run.conditional_data, \"msa_key_label_order.txt\"), 'w') as txt_labels:\n",
    "    for k, label_tensor in msa_key_label_order.items():\n",
    "        txt_labels.write(f\"{k} {label_tensor}\\n\")"
   ],
   "id": "bb230990f8c32233",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training of the model\n",
    "Select the models you would like to work with and prepare data for training and benchmarking if selected in configuration file. \n",
    "\n",
    "If you select a conditional model please see above cell and prepare labels for it.  "
   ],
   "id": "b531eb8f7d84c621"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\" Configure model \"\"\"\n",
    "available_models = [\"vae\", \"vae_conditional\"]\n",
    "model_selected = available_models[0]\n",
    "\n",
    "\"\"\" Prepare data for training \"\"\"\n",
    "setup_train(run, model_selected)"
   ],
   "id": "720546aea5acc95c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Train VAE with a preprocessed MSA\n",
    ":param conf_path: path to the configuration file of the project\n",
    ":return:\n",
    "\"\"\"\n",
    "# logging\n",
    "train_log = open(os.path.join(run.logs, 'train_log.txt'), \"w\")\n",
    "model = Train(run)\n",
    "with Capturing() as output_train:\n",
    "    model.train()\n",
    "train_log.write(\" Training \\n\" + \"\\n\".join(output_train) + f'\\n{\"=\" * 80}\\n')\n",
    "train_log.close()"
   ],
   "id": "8724f65afd8ddeb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\" Do I work with conditional model? \"\"\"\n",
    "\n",
    "load_from_pkl(os.path.join(run.model, \"model_params.pkl\"))['model']"
   ],
   "id": "81926bf7b9c11c3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generative Capacity evaluation\n",
    "If you wish to obtain estimates of your model's generative capacity, please set run_capacity_test to true in the configuration file before starting the training.\n",
    "\n",
    "This test generates first and second order statistics of your model."
   ],
   "id": "42662ecb12708231"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b4729b447cf64358"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from notebooks.minimal_version.benchmark import Benchmarker\n",
    "\"\"\" \n",
    "Benchmark model \n",
    "How well our model can regenarate sequences in various sets (positive, negative and training)\n",
    "\"\"\"\n",
    "N_SAMPLES = 500\n",
    "\n",
    "Benchmarker(run, samples=N_SAMPLES).bench_dataset()"
   ],
   "id": "c0880b617c49958a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Projection to the latent space\n",
    "Project all points to the latent space and make custom selection of sequences"
   ],
   "id": "9f1d6d94fb6b300f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from notebooks.minimal_version.latent_space import LatentSpace\n",
    "\n",
    "# Projection to the latent space\n",
    "# Show your queries \n",
    "latent_space = LatentSpace(run)\n",
    "msa_embeddings = latent_space.msa_embeddings[\"mus\"]\n",
    "\n",
    "query_coords = latent_space.key_to_embedding(run.query)\n",
    "\n",
    "custom_sequences = []  # give keys to MSA and embed them to the latent space\n",
    "\n",
    "\n",
    "fig_lat, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(msa_embeddings[:, 0], msa_embeddings[:, 1], '.', alpha=0.1, markersize=3, )\n",
    "ax.plot(query_coords[0], query_coords[1], '.', color='red')\n",
    "\n",
    "# Project \n",
    "for seq_id, seq_mu in enumerate(custom_sequences):\n",
    "    ax.plot(seq_mu[0], seq_mu[1], '.', color='black', alpha=1, markersize=5,\n",
    "            label='({})'.format(seq_id))\n",
    "    \n",
    "    ax.annotate(str(seq_id), (seq_mu[0], seq_mu[1]))\n",
    "ax.set_xlabel(\"$Z_1$\")\n",
    "ax.set_ylabel(\"$Z_2$\")\n",
    "fig_lat.savefig(os.path.join(run.results, \"latent_space.png\"))"
   ],
   "id": "7bf2dbc32c8ec2e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generate ancestors\n",
    "Using straight evolution protocol  "
   ],
   "id": "be569ab1800fe63e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from notebooks.minimal_version.evolution_protocols.straight_evolution import StraightEvolution\n",
    "\n",
    "protocol = StraightEvolution(run)\n",
    "ancestors = protocol.get_ancestors(profile=False)"
   ],
   "id": "6c0529b864b09d2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Project the conditional labels on the latent space",
   "id": "40b22c4875e9dad1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from notebooks.minimal_version.latent_space import LatentSpace\n",
    "\n",
    "assert load_from_pkl(os.path.join(run.model, \"model_params.pkl\"))['model'] == \"vae_conditional\"\n",
    "\n",
    "labels_file_pkl = os.path.join(run.conditional_data, \"conditional_labels_to_categories.pkl\")\n",
    "msa_idx_to_label = load_from_pkl(labels_file_pkl)\n",
    "# Projection to the latent space\n",
    "# Show your queries \n",
    "latent_space = LatentSpace(run)\n",
    "msa_embeddings = latent_space.msa_embeddings[\"mus\"]\n",
    "\n",
    "query_coords = latent_space.key_to_embedding(run.query)\n",
    "\n",
    "custom_sequences = []  # give keys to MSA and embed them to the latent space\n",
    "\n",
    "fig_lat, ax = plt.subplots(1, 1)\n",
    "\n",
    "colors_map = ['r', 'b', 'g']\n",
    "label_values = [label.argmax() for label in msa_idx_to_label]\n",
    "\n",
    "# Convert one-hot tensors to tuples for mapping to colors\n",
    "colors = [colors_map[one_hot] for one_hot in label_values]\n",
    "\n",
    "ax.scatter(msa_embeddings[:, 0], msa_embeddings[:, 1], c=colors, alpha=0.1)\n",
    "ax.plot(query_coords[0], query_coords[1], '.', color='red')\n",
    "\n",
    "# Project \n",
    "for seq_id, seq_mu in enumerate(custom_sequences):\n",
    "    ax.plot(seq_mu[0], seq_mu[1], '.', color='black', alpha=1, markersize=5,\n",
    "            label='({})'.format(seq_id))\n",
    "    \n",
    "    ax.annotate(str(seq_id), (seq_mu[0], seq_mu[1]))\n",
    "ax.set_xlabel(\"$Z_1$\")\n",
    "ax.set_ylabel(\"$Z_2$\")\n",
    "fig_lat.savefig(os.path.join(run.results, \"conditional_latent_space.png\"), dpi=500)"
   ],
   "id": "53dcfe07adb574d8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
